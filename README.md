# Lottus IA

`Lottus IA` Ã© um assistente conversacional baseado em IA para bibliotecas escolares, capaz de:

* Classificar a intenÃ§Ã£o da pergunta do usuÃ¡rio.
* Consultar dados de um banco de dados quando necessÃ¡rio.
* Gerar respostas contextuais ou gerais, dependendo da intenÃ§Ã£o.
* Integrar-se com modelos de IA (como Gemini, OpenAI, Claude) via LangChain.

A aplicaÃ§Ã£o Ã© modular e configurÃ¡vel, permitindo alterar o modelo de IA e parÃ¢metros diretamente pelo arquivo `.env`.

---

## ğŸ”¹ Funcionalidades

1. **ClassificaÃ§Ã£o de intenÃ§Ã£o**
   Identifica se a pergunta do usuÃ¡rio Ã©:

   * Fora do contexto;
   * Conversa geral;
   * Consulta especÃ­fica ao banco de dados.

2. **Consulta ao banco de dados**
   Para perguntas de consulta, a aplicaÃ§Ã£o busca os dados necessÃ¡rios e gera a resposta apropriada.

3. **GeraÃ§Ã£o de resposta com IA**
   Para perguntas fora do contexto ou conversas gerais, utiliza um modelo de IA para criar respostas inteligentes.

4. **Logs centralizados**
   Toda interaÃ§Ã£o Ã© registrada com `logger` para monitoramento e debugging.

---

## ğŸ”¹ Tecnologias

* **Python 3.10+**
* **LangChain** (`langchain.chat_models`, `langchain_core.prompts`)
* **dotenv** para variÃ¡veis de ambiente
* **Banco de dados** (via mÃ³dulo `Banco` â€” MySQL ou similar)
* **Logging** personalizado via `logger`

---

## ğŸ”¹ Estrutura do projeto

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classificador.py           # Classifica a intenÃ§Ã£o da pergunta
â”‚   â”œâ”€â”€ banco.py                   # Interface com o banco de dados
â”‚   â”œâ”€â”€ chain_resposta.py          # Gera resposta final a partir dos dados
â”‚   â”œâ”€â”€ prompts.py                 # Templates de prompts
â”‚   â””â”€â”€ logger.py                  # ConfiguraÃ§Ã£o de logs
â”‚
â”œâ”€â”€ .env                           # VariÃ¡veis de ambiente
â”œâ”€â”€ main.py                        # Ponto de entrada da aplicaÃ§Ã£o
â””â”€â”€ README.md
```

---

## ğŸ”¹ ConfiguraÃ§Ã£o

1. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

2. Configure o arquivo `.env` na raiz do projeto:

3. Inicie a aplicaÃ§Ã£o com o comando 

```python
flask run
```

Fluxo de funcionamento:

1. Recebe a pergunta do usuÃ¡rio.
2. Classifica a intenÃ§Ã£o com `classificar_intencao`.
3. Se for conversa geral ou fora do contexto, gera resposta com prompt padrÃ£o.
4. Se for intenÃ§Ã£o de banco, consulta `Banco` e gera resposta com `gerar_resposta`.
5. Retorna a resposta final ao usuÃ¡rio.

---

## ğŸ”¹ Logs

Todas as interaÃ§Ãµes sÃ£o registradas usando o mÃ³dulo `logger`:

```python
logger.info("Pergunta recebida: ...")
logger.info("IntenÃ§Ã£o classificada: ...")
logger.info("Dados retornados: ...")
```

---

## ğŸ”¹ PersonalizaÃ§Ã£o

* Para adicionar novas intenÃ§Ãµes ou consultas ao banco, edite `classificador.py` e `banco.py`.
* Para criar novos templates de prompt, edite `prompts.py`.
* Para alterar o modelo de IA, configure no `.env`:

```
ANSWER_IA_MODEL=gemini-2.0-flash
ANSWER_IA_PROVIDER=google_genai
ANSWER_IA_TEMPERATURE=0.2
```




